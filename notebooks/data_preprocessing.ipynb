{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Joeri R. Hermans                    \n",
    "*Departement of Data Science & Knowledge Engineering*          \n",
    "*Maastricht University, The Netherlands*  \n",
    "\n",
    "After the data exploration, we now have a rough idea which features to use, and how to normalize them. Before we actually train the models, we need to normalize them. This is the intent of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "import pyspark\n",
    "\n",
    "from distkeras.transformers import *\n",
    "from distkeras.utils import *\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "from pyspark.sql.functions import mean\n",
    "from pyspark.sql.functions import stddev_pop\n",
    "from pyspark.sql.functions import min\n",
    "from pyspark.sql.functions import max\n",
    "\n",
    "# Use the DataBricks CSV reader, this has some nice functionality regarding invalid values.\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-csv_2.10:1.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of desired executors: 20\n",
      "Number of desired processes / executor: 1\n",
      "Total number of workers: 20\n"
     ]
    }
   ],
   "source": [
    "# Modify these variables according to your needs.\n",
    "application_name = \"CMS Event Preprocessing for ML jobs\"\n",
    "using_spark_2 = False\n",
    "local = False\n",
    "path_data = \"data/events.csv\"\n",
    "if local:\n",
    "    # Tell master to use local resources.\n",
    "    master = \"local[*]\"\n",
    "    num_processes = 3\n",
    "    num_executors = 1\n",
    "else:\n",
    "    # Tell master to use YARN.\n",
    "    master = \"yarn-client\"\n",
    "    num_executors = 20\n",
    "    num_processes = 1\n",
    "\n",
    "# This variable is derived from the number of cores and executors,\n",
    "# and will be used to assign the number of model trainers.\n",
    "num_workers = num_executors * num_processes\n",
    "\n",
    "print(\"Number of desired executors: \" + `num_executors`)\n",
    "print(\"Number of desired processes / executor: \" + `num_processes`)\n",
    "print(\"Total number of workers: \" + `num_workers`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do not change anything here.\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", application_name)\n",
    "conf.set(\"spark.master\", master)\n",
    "conf.set(\"spark.executor.cores\", `num_processes`)\n",
    "conf.set(\"spark.executor.instances\", `num_executors`)\n",
    "conf.set(\"spark.executor.memory\", \"4g\")\n",
    "conf.set(\"spark.locality.wait\", \"0\")\n",
    "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\");\n",
    "\n",
    "# Check if the user is running Spark 2.0 +\n",
    "if using_spark_2:\n",
    "    sc = SparkSession.builder.config(conf=conf) \\\n",
    "            .appName(application_name) \\\n",
    "            .getOrCreate()\n",
    "else:\n",
    "    # Create the Spark context.\n",
    "    sc = SparkContext(conf=conf)\n",
    "    # Add the missing imports\n",
    "    from pyspark import SQLContext\n",
    "    sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of events: 1747413\n"
     ]
    }
   ],
   "source": [
    "# Check if we are using Spark 2.0\n",
    "if using_spark_2:\n",
    "    reader = sc\n",
    "else:\n",
    "    reader = sqlContext\n",
    "# Read the dataset.\n",
    "dataset = reader.read.format('com.databricks.spark.csv') \\\n",
    "                .options(header='true', inferSchema='true') \\\n",
    "                .load(path_data)\n",
    "# Repartition the dataset.\n",
    "dataset = dataset.repartition(num_workers)\n",
    "dataset.cache()\n",
    "\n",
    "# Count the total number of events.\n",
    "print(\"Total number of events: \" + str(dataset.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- run: integer (nullable = true)\n",
      " |-- evt: integer (nullable = true)\n",
      " |-- lumi: integer (nullable = true)\n",
      " |-- charge: integer (nullable = true)\n",
      " |-- chi2: double (nullable = true)\n",
      " |-- ndof: double (nullable = true)\n",
      " |-- normalizedChi2: double (nullable = true)\n",
      " |-- qoverp: double (nullable = true)\n",
      " |-- theta: double (nullable = true)\n",
      " |-- lambda: double (nullable = true)\n",
      " |-- dxy: double (nullable = true)\n",
      " |-- d0: double (nullable = true)\n",
      " |-- dsz: double (nullable = true)\n",
      " |-- dz: double (nullable = true)\n",
      " |-- p: double (nullable = true)\n",
      " |-- pt: double (nullable = true)\n",
      " |-- px: double (nullable = true)\n",
      " |-- py: double (nullable = true)\n",
      " |-- pz: double (nullable = true)\n",
      " |-- eta: double (nullable = true)\n",
      " |-- phi: double (nullable = true)\n",
      " |-- vx: double (nullable = true)\n",
      " |-- vy: double (nullable = true)\n",
      " |-- vz: double (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We are only interested in the following features\n",
    "# Extract columns which are interesting to us.\n",
    "features = dataset.columns\n",
    "del features[0]\n",
    "features.remove('TrackId')\n",
    "# Remove the pixel detector hits.\n",
    "for i in range(0, 5):\n",
    "    features.remove(\"pix_\" + str(i) + \"_x\")\n",
    "    features.remove(\"pix_\" + str(i) + \"_y\")\n",
    "    features.remove(\"pix_\" + str(i) + \"_z\")\n",
    "# Remove the silicon detector hits.\n",
    "for i in range(0, 50):\n",
    "    features.remove(\"sis_\" + str(i) + \"_x\")\n",
    "    features.remove(\"sis_\" + str(i) + \"_y\")\n",
    "    features.remove(\"sis_\" + str(i) + \"_z\")\n",
    "# Filter them from the dataset.\n",
    "dataset = dataset.select(features)\n",
    "dataset.cache()\n",
    "# Remove other columns from the feature list.\n",
    "features.remove('run')\n",
    "features.remove('evt')\n",
    "features.remove('lumi')\n",
    "features.remove('label')\n",
    "features.remove('ndof')\n",
    "features.remove('chi2')\n",
    "features.remove('normalizedChi2')\n",
    "\n",
    "# Show the new schema.\n",
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Wjet',\n",
       " u'SMS-T1tttt_mGl',\n",
       " u'DisplacedSUSY_stopToBottom',\n",
       " u'RSGravitonToGaGa',\n",
       " u'PhiToMuMu',\n",
       " u'H125GGgluonfusion']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch all distinct labels.\n",
    "labels = dataset.select(\"label\").distinct().collect()\n",
    "num_labels = len(labels)\n",
    "for i in range(0, num_labels):\n",
    "    labels[i] = labels[i].asDict()['label']\n",
    "    \n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- charge\n",
      "- qoverp\n",
      "- theta\n",
      "- lambda\n",
      "- dxy\n",
      "- d0\n",
      "- dsz\n",
      "- dz\n",
      "- p\n",
      "- pt\n",
      "- px\n",
      "- py\n",
      "- pz\n",
      "- eta\n",
      "- phi\n",
      "- vx\n",
      "- vy\n",
      "- vz\n"
     ]
    }
   ],
   "source": [
    "# Display the features we will use in our models\n",
    "for f in features:\n",
    "    print(\"- \" + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "charge: -1 - 1\n",
      "qoverp: -6.372203513 - 6.29370927046\n",
      "theta: 0.063404051408 - 3.09221749952\n",
      "lambda: -1.52142117272 - 1.50739227539\n",
      "dxy: -122.304409123 - 138.917929453\n",
      "d0: -138.917929453 - 122.304409123\n",
      "dsz: -189.87788268 - 191.659099905\n",
      "dz: -543.282959864 - 1289.63191789\n",
      "p: 0.156931585433 - 418632.894996\n",
      "pt: 0.0601777821704 - 412066.929499\n",
      "px: -258788.734375 - 304492.25\n",
      "py: -54888.5 - 320667.34375\n",
      "pz: -214375.046875 - 70553.8359375\n",
      "eta: -3.70125192813 - 3.45103961308\n",
      "phi: -3.14158964157 - 3.14159178734\n",
      "vx: -120.217697144 - 121.903030396\n",
      "vy: -113.07585907 - 116.766670227\n",
      "vz: -543.150695801 - 1292.33703613\n"
     ]
    }
   ],
   "source": [
    "# Collect the min-max value for every feature.\n",
    "for f in features:\n",
    "    r = dataset.select(pyspark.sql.functions.min(f),pyspark.sql.functions.max(f)).collect()[0].asDict()\n",
    "    min = r['min(' + f + ')']\n",
    "    max = r['max(' + f + ')']\n",
    "    print(f + \": \" + str(min) + \" - \" + str(max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- charge: long (nullable = true)\n",
      " |-- qoverp: double (nullable = true)\n",
      " |-- theta: double (nullable = true)\n",
      " |-- lambda: double (nullable = true)\n",
      " |-- dxy: double (nullable = true)\n",
      " |-- d0: double (nullable = true)\n",
      " |-- dsz: double (nullable = true)\n",
      " |-- dz: double (nullable = true)\n",
      " |-- p: double (nullable = true)\n",
      " |-- pt: double (nullable = true)\n",
      " |-- px: double (nullable = true)\n",
      " |-- py: double (nullable = true)\n",
      " |-- pz: double (nullable = true)\n",
      " |-- eta: double (nullable = true)\n",
      " |-- phi: double (nullable = true)\n",
      " |-- vx: double (nullable = true)\n",
      " |-- vy: double (nullable = true)\n",
      " |-- vz: double (nullable = true)\n",
      " |-- run: long (nullable = true)\n",
      " |-- evt: long (nullable = true)\n",
      " |-- lumi: long (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- qoverp_norm: double (nullable = true)\n",
      " |-- theta_norm: double (nullable = true)\n",
      " |-- lambda_norm: double (nullable = true)\n",
      " |-- dxy_norm: double (nullable = true)\n",
      " |-- d0_norm: double (nullable = true)\n",
      " |-- dsz_norm: double (nullable = true)\n",
      " |-- dz_norm: double (nullable = true)\n",
      " |-- p_norm: double (nullable = true)\n",
      " |-- pt_norm: double (nullable = true)\n",
      " |-- px_norm: double (nullable = true)\n",
      " |-- py_norm: double (nullable = true)\n",
      " |-- pz_norm: double (nullable = true)\n",
      " |-- eta_norm: double (nullable = true)\n",
      " |-- phi_norm: double (nullable = true)\n",
      " |-- vx_norm: double (nullable = true)\n",
      " |-- vy_norm: double (nullable = true)\n",
      " |-- vz_norm: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now, let's normalize all the features according to the plan described above.\n",
    "# All normalized features will have a '_norm' suffix.\n",
    "# However, it is possible that certain events contain outliers, but we will ignore these,\n",
    "# and check how it goes.\n",
    "# For the normalization we use the metrics obtained in the Data Exploration notebook.\n",
    "columns = list(features)\n",
    "columns.append('run')\n",
    "columns.append('evt')\n",
    "columns.append('lumi')\n",
    "columns.append('label')\n",
    "dataset = dataset.select(columns)\n",
    "\n",
    "# Normalize qoverp.\n",
    "t = MinMaxTransformer(o_min=-7.0, o_max=7.0, n_min=-1.0, n_max=1.0, input_col=\"qoverp\", output_col=\"qoverp_norm\", is_vector=False)\n",
    "dataset = t.transform(dataset)\n",
    "# Normalize theta.\n",
    "t = MinMaxTransformer(o_min=-3.5, o_max=3.5, n_min=-1.0, n_max=1.0, input_col=\"theta\", output_col=\"theta_norm\", is_vector=False)\n",
    "dataset = t.transform(dataset)\n",
    "# Normalize lambda.\n",
    "t = MinMaxTransformer(o_min=-3.5, o_max=3.5, n_min=-1.0, n_max=1.0, input_col=\"lambda\", output_col=\"lambda_norm\", is_vector=False)\n",
    "dataset = t.transform(dataset)\n",
    "# Normalize dxy\n",
    "t = MinMaxTransformer(o_min=-35, o_max=35, n_min=-1.0, n_max=1.0, input_col=\"dxy\", output_col=\"dxy_norm\", is_vector=False)\n",
    "dataset = t.transform(dataset)\n",
    "# Normalize d0\n",
    "t = MinMaxTransformer(o_min=-35, o_max=35, n_min=-1.0, n_max=1.0, input_col=\"d0\", output_col=\"d0_norm\", is_vector=False)\n",
    "dataset = t.transform(dataset)\n",
    "# Normalize dsz\n",
    "t = MinMaxTransformer(o_min=-60, o_max=60, n_min=-1.0, n_max=1.0, input_col=\"dsz\", output_col=\"dsz_norm\", is_vector=False)\n",
    "dataset = t.transform(dataset)\n",
    "# Normalize dz\n",
    "t = MinMaxTransformer(o_min=-120, o_max=120, n_min=-1.0, n_max=1.0, input_col=\"dz\", output_col=\"dz_norm\", is_vector=False)\n",
    "dataset = t.transform(dataset)\n",
    "# Normalize p\n",
    "t = MinMaxTransformer(o_min=0, o_max=200, n_min=0, n_max=1.0, input_col=\"p\", output_col=\"p_norm\", is_vector=False)\n",
    "dataset = t.transform(dataset)\n",
    "# Normalize pt\n",
    "t = MinMaxTransformer(o_min=0, o_max=200, n_min=0, n_max=1.0, input_col=\"pt\", output_col=\"pt_norm\", is_vector=False)\n",
    "dataset = t.transform(dataset)\n",
    "# Normalize px\n",
    "t = MinMaxTransformer(o_min=-5000, o_max=5000, n_min=-5.0, n_max=5.0, input_col=\"px\", output_col=\"px_norm\", is_vector=False)\n",
    "dataset = t.transform(dataset)\n",
    "# Normalize py\n",
    "t = MinMaxTransformer(o_min=-3000.0, o_max=3000.0, n_min=-3.0, n_max=3.0, input_col=\"py\", output_col=\"py_norm\", is_vector=False)\n",
    "dataset = t.transform(dataset)\n",
    "# Normalize pz\n",
    "t = MinMaxTransformer(o_min=-2500.0, o_max=2500.0, n_min=-2.5, n_max=2.5, input_col=\"pz\", output_col=\"pz_norm\", is_vector=False)\n",
    "dataset = t.transform(dataset)\n",
    "# Normalize eta\n",
    "t = MinMaxTransformer(o_min=-4.0, o_max=4.0, n_min=-1, n_max=1, input_col=\"eta\", output_col=\"eta_norm\", is_vector=False)\n",
    "dataset = t.transform(dataset)\n",
    "# Normalize phi\n",
    "t = MinMaxTransformer(o_min=-4.0, o_max=4.0, n_min=-1, n_max=1, input_col=\"phi\", output_col=\"phi_norm\", is_vector=False)\n",
    "dataset = t.transform(dataset)\n",
    "# Normalize vx\n",
    "t = MinMaxTransformer(o_min=-35.0, o_max=35.0, n_min=-1, n_max=1, input_col=\"vx\", output_col=\"vx_norm\", is_vector=False)\n",
    "dataset = t.transform(dataset)\n",
    "# Normalize vy\n",
    "t = MinMaxTransformer(o_min=-14.0, o_max=14.0, n_min=-1, n_max=1, input_col=\"vy\", output_col=\"vy_norm\", is_vector=False)\n",
    "dataset = t.transform(dataset)\n",
    "# Normalize vz\n",
    "t = MinMaxTransformer(o_min=-120.0, o_max=120.0, n_min=-1, n_max=1, input_col=\"vz\", output_col=\"vz_norm\", is_vector=False)\n",
    "dataset = t.transform(dataset)\n",
    "\n",
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# In order to have a comparison, let use vectorize the above features to establish a baseline for learning.\n",
    "vector_assembler = VectorAssembler(inputCols=features, outputCol=\"features_raw\")\n",
    "dataset = vector_assembler.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct an vector with the names of the normalized columns.\n",
    "features_normalized = [x + \"_norm\" for x in features]\n",
    "features_normalized.remove('charge_norm')\n",
    "features_normalized.append('charge')\n",
    "# Vectorize the normalized features.\n",
    "vector_assembler = VectorAssembler(inputCols=features_normalized, outputCol=\"features_normalized_raw\")\n",
    "dataset = vector_assembler.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- charge: long (nullable = true)\n",
      " |-- qoverp: double (nullable = true)\n",
      " |-- theta: double (nullable = true)\n",
      " |-- lambda: double (nullable = true)\n",
      " |-- dxy: double (nullable = true)\n",
      " |-- d0: double (nullable = true)\n",
      " |-- dsz: double (nullable = true)\n",
      " |-- dz: double (nullable = true)\n",
      " |-- p: double (nullable = true)\n",
      " |-- pt: double (nullable = true)\n",
      " |-- px: double (nullable = true)\n",
      " |-- py: double (nullable = true)\n",
      " |-- pz: double (nullable = true)\n",
      " |-- eta: double (nullable = true)\n",
      " |-- phi: double (nullable = true)\n",
      " |-- vx: double (nullable = true)\n",
      " |-- vy: double (nullable = true)\n",
      " |-- vz: double (nullable = true)\n",
      " |-- run: long (nullable = true)\n",
      " |-- evt: long (nullable = true)\n",
      " |-- lumi: long (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- qoverp_norm: double (nullable = true)\n",
      " |-- theta_norm: double (nullable = true)\n",
      " |-- lambda_norm: double (nullable = true)\n",
      " |-- dxy_norm: double (nullable = true)\n",
      " |-- d0_norm: double (nullable = true)\n",
      " |-- dsz_norm: double (nullable = true)\n",
      " |-- dz_norm: double (nullable = true)\n",
      " |-- p_norm: double (nullable = true)\n",
      " |-- pt_norm: double (nullable = true)\n",
      " |-- px_norm: double (nullable = true)\n",
      " |-- py_norm: double (nullable = true)\n",
      " |-- pz_norm: double (nullable = true)\n",
      " |-- eta_norm: double (nullable = true)\n",
      " |-- phi_norm: double (nullable = true)\n",
      " |-- vx_norm: double (nullable = true)\n",
      " |-- vy_norm: double (nullable = true)\n",
      " |-- vz_norm: double (nullable = true)\n",
      " |-- features_raw: vector (nullable = true)\n",
      " |-- features_normalized_raw: vector (nullable = true)\n",
      "\n",
      "Deleted events.parquet\n"
     ]
    }
   ],
   "source": [
    "# Check if all columns are present.\n",
    "dataset.printSchema()\n",
    "# Remove the old Parquet file if it exists.\n",
    "!hdfs dfs -rm -r events.parquet\n",
    "# Write to a Parquet file so we don't have to execute the above when we run this notebook again.\n",
    "dataset.write.parquet(\"events.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- charge: long (nullable = true)\n",
      " |-- qoverp: double (nullable = true)\n",
      " |-- theta: double (nullable = true)\n",
      " |-- lambda: double (nullable = true)\n",
      " |-- dxy: double (nullable = true)\n",
      " |-- d0: double (nullable = true)\n",
      " |-- dsz: double (nullable = true)\n",
      " |-- dz: double (nullable = true)\n",
      " |-- p: double (nullable = true)\n",
      " |-- pt: double (nullable = true)\n",
      " |-- px: double (nullable = true)\n",
      " |-- py: double (nullable = true)\n",
      " |-- pz: double (nullable = true)\n",
      " |-- eta: double (nullable = true)\n",
      " |-- phi: double (nullable = true)\n",
      " |-- vx: double (nullable = true)\n",
      " |-- vy: double (nullable = true)\n",
      " |-- vz: double (nullable = true)\n",
      " |-- run: long (nullable = true)\n",
      " |-- evt: long (nullable = true)\n",
      " |-- lumi: long (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- qoverp_norm: double (nullable = true)\n",
      " |-- theta_norm: double (nullable = true)\n",
      " |-- lambda_norm: double (nullable = true)\n",
      " |-- dxy_norm: double (nullable = true)\n",
      " |-- d0_norm: double (nullable = true)\n",
      " |-- dsz_norm: double (nullable = true)\n",
      " |-- dz_norm: double (nullable = true)\n",
      " |-- p_norm: double (nullable = true)\n",
      " |-- pt_norm: double (nullable = true)\n",
      " |-- px_norm: double (nullable = true)\n",
      " |-- py_norm: double (nullable = true)\n",
      " |-- pz_norm: double (nullable = true)\n",
      " |-- eta_norm: double (nullable = true)\n",
      " |-- phi_norm: double (nullable = true)\n",
      " |-- vx_norm: double (nullable = true)\n",
      " |-- vy_norm: double (nullable = true)\n",
      " |-- vz_norm: double (nullable = true)\n",
      " |-- features_raw: vector (nullable = true)\n",
      " |-- features_normalized_raw: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the parquet file.\n",
    "dataset = sqlContext.read.parquet(\"events.parquet\")\n",
    "# Repartition the dataset.\n",
    "dataset = dataset.repartition(num_workers)\n",
    "dataset.cache()\n",
    "# Print the schema.\n",
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add binary labels for binary classification experiments.\n",
    "for i in range(0, num_labels):\n",
    "    label = labels[i]\n",
    "    input_column = \"label\"\n",
    "    output_column = \"label_binary_\" + str(i)\n",
    "    t = BinaryLabelTransformer(input_column, output_column, label)\n",
    "    dataset = t.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- charge: long (nullable = true)\n",
      " |-- qoverp: double (nullable = true)\n",
      " |-- theta: double (nullable = true)\n",
      " |-- lambda: double (nullable = true)\n",
      " |-- dxy: double (nullable = true)\n",
      " |-- d0: double (nullable = true)\n",
      " |-- dsz: double (nullable = true)\n",
      " |-- dz: double (nullable = true)\n",
      " |-- p: double (nullable = true)\n",
      " |-- pt: double (nullable = true)\n",
      " |-- px: double (nullable = true)\n",
      " |-- py: double (nullable = true)\n",
      " |-- pz: double (nullable = true)\n",
      " |-- eta: double (nullable = true)\n",
      " |-- phi: double (nullable = true)\n",
      " |-- vx: double (nullable = true)\n",
      " |-- vy: double (nullable = true)\n",
      " |-- vz: double (nullable = true)\n",
      " |-- run: long (nullable = true)\n",
      " |-- evt: long (nullable = true)\n",
      " |-- lumi: long (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- qoverp_norm: double (nullable = true)\n",
      " |-- theta_norm: double (nullable = true)\n",
      " |-- lambda_norm: double (nullable = true)\n",
      " |-- dxy_norm: double (nullable = true)\n",
      " |-- d0_norm: double (nullable = true)\n",
      " |-- dsz_norm: double (nullable = true)\n",
      " |-- dz_norm: double (nullable = true)\n",
      " |-- p_norm: double (nullable = true)\n",
      " |-- pt_norm: double (nullable = true)\n",
      " |-- px_norm: double (nullable = true)\n",
      " |-- py_norm: double (nullable = true)\n",
      " |-- pz_norm: double (nullable = true)\n",
      " |-- eta_norm: double (nullable = true)\n",
      " |-- phi_norm: double (nullable = true)\n",
      " |-- vx_norm: double (nullable = true)\n",
      " |-- vy_norm: double (nullable = true)\n",
      " |-- vz_norm: double (nullable = true)\n",
      " |-- features_raw: vector (nullable = true)\n",
      " |-- features_normalized_raw: vector (nullable = true)\n",
      " |-- label_binary_0: vector (nullable = true)\n",
      " |-- label_binary_1: vector (nullable = true)\n",
      " |-- label_binary_2: vector (nullable = true)\n",
      " |-- label_binary_3: vector (nullable = true)\n",
      " |-- label_binary_4: vector (nullable = true)\n",
      " |-- label_binary_5: vector (nullable = true)\n",
      " |-- label_index: double (nullable = true)\n",
      " |-- label_multiclass: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Later, we also want to do multiclass classification.\n",
    "# For this, we first need to convert the labels to indexes,\n",
    "# and then one-hot encode them.\n",
    "\n",
    "# Convert labels to indexes.\n",
    "string_indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_index\")\n",
    "dataset = string_indexer.fit(dataset).transform(dataset)\n",
    "# One-hot encode the labels for multi-class classification.\n",
    "t = OneHotTransformer(input_col=\"label_index\", output_col=\"label_multiclass\", output_dim=num_labels)\n",
    "dataset = t.transform(dataset)\n",
    "\n",
    "# Check if all required columns have been added.\n",
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted events_processed.parquet\r\n"
     ]
    }
   ],
   "source": [
    "# Remove previous possible dataframe.\n",
    "!hdfs dfs -rm -r events_processed.parquet\n",
    "# Write the preprocessed dataframe to HDFS for later use.\n",
    "dataset.write.parquet(\"events_processed.parquet\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
